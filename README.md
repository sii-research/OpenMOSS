# OpenMOSS
OpenMOSS presents a collection of our research on LLMs, supported by SII, Fudan and Mosi.


# Projects

## MOSS-Audio
### MOSS: Text to Spoken Dialogue Generation
**Release Date**: June 2025
#### üîó Resources
- **ü§ó HuggingFace**: [MOSS-TTSD Models](https://huggingface.co/fnlp/MOSS-TTSD-v0.5)
- **üíª GitHub**: [Source Code & Implementation](https://github.com/OpenMOSS/MOSS-TTSD)
## MOSS-Video
Coming soon!
## MOSS-Robot
Coming soon!
## MOSS-Interp
### Language-Model-SAEs
`Language-Model-SAEs` is a comprehensive, **fully-distributed** framework designed for **training, analyzing and visualizing Sparse Autoencoders (SAEs)**, empowering scalable and systematic **Mechanistic Interpretability** research.
#### üîó Resources
- ü§ó HuggingFace: [Llama Scope](https://huggingface.co/fnlp/Llama-Scope)
- üåê Neuronpedia: [Llama Scope Visualization](https://www.neuronpedia.org/llama-scope)
- üíª GitHub: [Language-Model-SAEs](https://github.com/OpenMOSS/Language-Model-SAEs)

# Research
## Embodied-AI
- [World-Aware-Planning](https://github.com/sjh0354/World-Aware-Planning)
- [Embodied-Planner-R1](https://github.com/OpenMOSS/Embodied-Planner-R1)
## NewArch
SII-OpenMOSS New Architecture Team explores new architectures and paradigms of LLMs, from the perspective of improving the long-context capability and efficiency of LLMs
- **ReAttention** [arXiv](https://arxiv.org/abs/2407.15176) [Github](https://github.com/OpenMOSS/ReAttention) ICLR 2025
  - A training-free approach that enables LLM to support an infinite context in length extrapolation with finite attention scope.
- **VideoRoPE** [arXiv](https://arxiv.org/abs/2502.05173) [Github](https://github.com/Wiselnn570/VideoRoPE) ICML 2025 Oral
  - A superior video rotary position embedding incorporating 3D structure, frequency allocation, spatial symmetry, and temporal scaling.
- **FouierAttention** [arXiv](https://arxiv.org/abs/2506.11886)
  - A training-free framework that exploits the heterogeneous roles of transformer head dimensions.
- **LongLLaDA** [arXiv](https://arxiv.org/abs/2506.14429) [Github](https://github.com/OpenMOSS/LongLLaDA)
  - The first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs.
- **Thus Spake Long-Context LLM** [arXiv](https://arxiv.org/abs/2502.17129) [Github](https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM)
  - A global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation.
